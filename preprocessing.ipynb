{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sriva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sriva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing synopses: 100%|██████████| 10813/10813 [00:11<00:00, 905.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    decadelong quest defeat demon king member hero...\n",
      "1    horrific alchemy experiment go wrong elric hou...\n",
      "2    two year passed following tendoshuus invasion ...\n",
      "3    hunter devote accomplishing hazardous task tra...\n",
      "4    oneyear hiatus shinpachi shimura return edo st...\n",
      "Name: synopsis, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"anime_dataset_cleaned.csv\")\n",
    "\n",
    "# Apply preprocessing to the 'synopsis' column with tqdm\n",
    "tqdm.pandas(desc=\"Preprocessing synopses\")  # Initialize tqdm for pandas\n",
    "df['synopsis'] = df['synopsis'].progress_apply(preprocess_text)\n",
    "\n",
    "# Example of the preprocessed synopses\n",
    "print(df['synopsis'].head())\n",
    "# Save the preprocessed dataset to a new CSV file\n",
    "df.to_csv(\"anime_dataset_preprocessed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
